{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from common.read_spectral_common import (\n",
    "    data_df_13_09_2022, \n",
    "    calculate_mutual_info_for_all, \n",
    "    generate_sample, create_X_train_Y_train, \n",
    "    mutual_info_regression, \n",
    "    get_max_bands, \n",
    "    get_bands_ix_from_mutual_info, \n",
    "    get_average_bands, \n",
    "    get_max_bands, \n",
    "    get_min_bands,\n",
    "    predict_using_neutral_network, \n",
    "    predict_using_random_forest, \n",
    "    predict_using_decision_tree,\n",
    "    get_full_path,\n",
    "    load_sklearn_model_to_file_by_cloudpickle,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import cloudpickle\n",
    "import common\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_value = \"N\"\n",
    "train_field = [\"T\", \"J\"]\n",
    "function_get = get_max_bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(109, 0.266475167231325),\n",
       " (45, 0.196122954304089),\n",
       " (102, 0.15157344875394374),\n",
       " (21, 0.12335508977432275),\n",
       " (41, 0.11416500875691415),\n",
       " (93, 0.11012437169631761),\n",
       " (69, 0.10751346860020483),\n",
       " (24, 0.09428707544226489),\n",
       " (7, 0.09332524049104407),\n",
       " (63, 0.09158496679653805),\n",
       " (114, 0.09080279753805254),\n",
       " (96, 0.08926160096411273),\n",
       " (42, 0.08649622610630026),\n",
       " (106, 0.08215284543165557),\n",
       " (2, 0.07718416278583629),\n",
       " (12, 0.07123400334383456),\n",
       " (48, 0.06622563941400728),\n",
       " (37, 0.05957749549469726),\n",
       " (87, 0.058034064567277444),\n",
       " (53, 0.05525325130245262),\n",
       " (51, 0.05333506237671726),\n",
       " (23, 0.04664410309205991),\n",
       " (79, 0.042472031146980616),\n",
       " (98, 0.04115870796455834),\n",
       " (70, 0.04053388407511482),\n",
       " (35, 0.03791249713776956),\n",
       " (72, 0.03536779679315005),\n",
       " (57, 0.0348409780245591),\n",
       " (16, 0.03396583913210005),\n",
       " (18, 0.029266313625623486),\n",
       " (119, 0.027582159624412794),\n",
       " (4, 0.023798618055023812),\n",
       " (9, 0.023696899803736127),\n",
       " (28, 0.02034555660044024),\n",
       " (59, 0.019915797186019102),\n",
       " (97, 0.019414841428535112),\n",
       " (121, 0.017988809176209708),\n",
       " (82, 0.016336854189142436),\n",
       " (77, 0.015289794074952567),\n",
       " (10, 0.014001875585842427),\n",
       " (116, 0.01388888888888895),\n",
       " (120, 0.01388888888888895),\n",
       " (19, 0.013029716164933092),\n",
       " (117, 0.012141832476338488),\n",
       " (1, 0.012002583967759328),\n",
       " (112, 0.011714780772912103),\n",
       " (84, 0.011555552010832137),\n",
       " (90, 0.01139433997535999),\n",
       " (100, 0.011252133506498119),\n",
       " (111, 0.011001562155064981),\n",
       " (68, 0.01088553222141364),\n",
       " (99, 0.01075052045187519),\n",
       " (5, 0.009016040519413515),\n",
       " (73, 0.00649986792341295),\n",
       " (94, 0.003531673780794886),\n",
       " (104, 0.003003177424849568),\n",
       " (8, 0.00226954697513726),\n",
       " (105, 0.0022545922090912995),\n",
       " (25, 0.001642359549235195),\n",
       " (103, 0.0015654473924247014),\n",
       " (0, 0.0),\n",
       " (3, 0.0),\n",
       " (6, 0.0),\n",
       " (11, 0.0),\n",
       " (13, 0.0),\n",
       " (14, 0.0),\n",
       " (15, 0.0),\n",
       " (17, 0.0),\n",
       " (20, 0.0),\n",
       " (22, 0.0),\n",
       " (26, 0.0),\n",
       " (27, 0.0),\n",
       " (29, 0.0),\n",
       " (30, 0.0),\n",
       " (31, 0.0),\n",
       " (32, 0.0),\n",
       " (33, 0.0),\n",
       " (34, 0.0),\n",
       " (36, 0.0),\n",
       " (38, 0.0),\n",
       " (39, 0.0),\n",
       " (40, 0.0),\n",
       " (43, 0.0),\n",
       " (44, 0.0),\n",
       " (46, 0.0),\n",
       " (47, 0.0),\n",
       " (49, 0.0),\n",
       " (50, 0.0),\n",
       " (52, 0.0),\n",
       " (54, 0.0),\n",
       " (55, 0.0),\n",
       " (56, 0.0),\n",
       " (58, 0.0),\n",
       " (60, 0.0),\n",
       " (61, 0.0),\n",
       " (62, 0.0),\n",
       " (64, 0.0),\n",
       " (65, 0.0),\n",
       " (66, 0.0),\n",
       " (67, 0.0),\n",
       " (71, 0.0),\n",
       " (74, 0.0),\n",
       " (75, 0.0),\n",
       " (76, 0.0),\n",
       " (78, 0.0),\n",
       " (80, 0.0),\n",
       " (81, 0.0),\n",
       " (83, 0.0),\n",
       " (85, 0.0),\n",
       " (86, 0.0),\n",
       " (88, 0.0),\n",
       " (89, 0.0),\n",
       " (91, 0.0),\n",
       " (92, 0.0),\n",
       " (95, 0.0),\n",
       " (101, 0.0),\n",
       " (107, 0.0),\n",
       " (108, 0.0),\n",
       " (110, 0.0),\n",
       " (113, 0.0),\n",
       " (115, 0.0),\n",
       " (118, 0.0)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_mutual_info_for_all(data_df_13_09_2022, target_value, train_field, function_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_ix = get_bands_ix_from_mutual_info(data_df_13_09_2022, -1, target_value, train_field, function_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bands_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bands_ix = filter(lambda data: data < 100, bands_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_ix = list(bands_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bands_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bands_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_now = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "NN_file_path = get_full_path(f\"../../model_saved/NN_save/NN_object/{'_'.join(train_field)}_predict_{'_'.join(target_value)}_{date_now}_using_{function_get.__name__}.pkl\")\n",
    "RF_file_path = get_full_path(f\"../../model_saved/RF_save/{'_'.join(train_field)}_predict_{'_'.join(target_value)}_{date_now}_using_{function_get.__name__}.pkl\")\n",
    "DT_file_path = get_full_path(f\"../../model_saved/DT_save/{'_'.join(train_field)}_predict_{'_'.join(target_value)}_{date_now}_using_{function_get.__name__}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | loss train: 5990.31640625\n",
      "Epoch: 100 | loss train: 5735.78466796875\n",
      "Epoch: 200 | loss train: 769.1438598632812\n",
      "Epoch: 300 | loss train: 763.0464477539062\n",
      "Epoch: 400 | loss train: 758.9034423828125\n",
      "Epoch: 500 | loss train: 753.021728515625\n",
      "Epoch: 600 | loss train: 735.8843383789062\n",
      "Epoch: 700 | loss train: 703.10009765625\n",
      "Epoch: 800 | loss train: 639.64599609375\n",
      "Epoch: 900 | loss train: 579.9301147460938\n",
      "Epoch: 1000 | loss train: 570.0950317382812\n",
      "Epoch: 1100 | loss train: 561.340087890625\n",
      "Epoch: 1200 | loss train: 551.3972778320312\n",
      "Epoch: 1300 | loss train: 539.7272338867188\n",
      "Epoch: 1400 | loss train: 526.0421752929688\n",
      "Epoch: 1500 | loss train: 508.0052490234375\n",
      "Epoch: 1600 | loss train: 487.75653076171875\n",
      "Epoch: 1700 | loss train: 469.1561584472656\n",
      "Epoch: 1800 | loss train: 453.9056701660156\n",
      "Epoch: 1900 | loss train: 438.4959716796875\n",
      "Epoch: 2000 | loss train: 426.2626037597656\n",
      "Epoch: 2100 | loss train: 413.84735107421875\n",
      "Epoch: 2200 | loss train: 405.3719787597656\n",
      "Epoch: 2300 | loss train: 392.10406494140625\n",
      "Epoch: 2400 | loss train: 382.4945373535156\n",
      "Epoch: 2500 | loss train: 372.6022033691406\n",
      "Epoch: 2600 | loss train: 363.20648193359375\n",
      "Epoch: 2700 | loss train: 355.32220458984375\n",
      "Epoch: 2800 | loss train: 348.7676696777344\n",
      "Epoch: 2900 | loss train: 337.8095397949219\n",
      "Epoch: 3000 | loss train: 328.9367980957031\n",
      "Epoch: 3100 | loss train: 326.1065368652344\n",
      "Epoch: 3200 | loss train: 313.93841552734375\n",
      "Epoch: 3300 | loss train: 307.3941955566406\n",
      "Epoch: 3400 | loss train: 299.55743408203125\n",
      "Epoch: 3500 | loss train: 291.0691223144531\n",
      "Epoch: 3600 | loss train: 286.1542663574219\n",
      "Epoch: 3700 | loss train: 277.19598388671875\n",
      "Epoch: 3800 | loss train: 270.0016784667969\n",
      "Epoch: 3900 | loss train: 265.8339538574219\n",
      "Epoch: 4000 | loss train: 274.3283996582031\n",
      "Epoch: 4100 | loss train: 254.0946044921875\n",
      "Epoch: 4200 | loss train: 248.98802185058594\n",
      "Epoch: 4300 | loss train: 239.5006103515625\n",
      "Epoch: 4400 | loss train: 237.7775421142578\n",
      "Epoch: 4500 | loss train: 230.75013732910156\n",
      "Epoch: 4600 | loss train: 227.98704528808594\n",
      "Epoch: 4700 | loss train: 231.19894409179688\n",
      "Epoch: 4800 | loss train: 220.51451110839844\n",
      "Epoch: 4900 | loss train: 220.409423828125\n",
      "Epoch: 5000 | loss train: 210.628662109375\n",
      "Epoch: 5100 | loss train: 208.5556182861328\n",
      "Epoch: 5200 | loss train: 208.2183380126953\n",
      "Epoch: 5300 | loss train: 206.1219024658203\n",
      "Epoch: 5400 | loss train: 197.49842834472656\n",
      "Epoch: 5500 | loss train: 195.9706268310547\n",
      "Epoch: 5600 | loss train: 191.76998901367188\n",
      "Epoch: 5700 | loss train: 197.61973571777344\n",
      "Epoch: 5800 | loss train: 192.38958740234375\n",
      "Epoch: 5900 | loss train: 189.0493621826172\n",
      "Epoch: 6000 | loss train: 187.28858947753906\n",
      "Epoch: 6100 | loss train: 187.7665557861328\n",
      "Epoch: 6200 | loss train: 172.4796142578125\n",
      "Epoch: 6300 | loss train: 178.701171875\n",
      "Epoch: 6400 | loss train: 179.7846221923828\n",
      "Epoch: 6500 | loss train: 165.898193359375\n",
      "Epoch: 6600 | loss train: 165.1167449951172\n",
      "Epoch: 6700 | loss train: 159.33184814453125\n",
      "Epoch: 6800 | loss train: 156.4873809814453\n",
      "Epoch: 6900 | loss train: 163.3220672607422\n",
      "Epoch: 7000 | loss train: 158.4371795654297\n",
      "Epoch: 7100 | loss train: 150.81129455566406\n",
      "Epoch: 7200 | loss train: 155.0743865966797\n",
      "Epoch: 7300 | loss train: 152.56845092773438\n",
      "Epoch: 7400 | loss train: 158.63035583496094\n",
      "Epoch: 7500 | loss train: 140.2681121826172\n",
      "Epoch: 7600 | loss train: 139.76312255859375\n",
      "Epoch: 7700 | loss train: 137.5083770751953\n",
      "Epoch: 7800 | loss train: 142.91400146484375\n",
      "Epoch: 7900 | loss train: 145.84339904785156\n",
      "Epoch: 8000 | loss train: 130.33456420898438\n",
      "Epoch: 8100 | loss train: 130.51808166503906\n",
      "Epoch: 8200 | loss train: 156.16065979003906\n",
      "Epoch: 8300 | loss train: 131.2559051513672\n",
      "Epoch: 8400 | loss train: 124.54850769042969\n",
      "Epoch: 8500 | loss train: 124.71419525146484\n",
      "Epoch: 8600 | loss train: 145.360595703125\n",
      "Epoch: 8700 | loss train: 132.95309448242188\n",
      "Epoch: 8800 | loss train: 127.36419677734375\n",
      "Epoch: 8900 | loss train: 121.3515396118164\n",
      "Epoch: 9000 | loss train: 113.48674774169922\n",
      "Epoch: 9100 | loss train: 125.06574249267578\n",
      "Epoch: 9200 | loss train: 127.1180191040039\n",
      "Epoch: 9300 | loss train: 120.44921112060547\n",
      "Epoch: 9400 | loss train: 120.55976104736328\n",
      "Epoch: 9500 | loss train: 130.29818725585938\n",
      "Epoch: 9600 | loss train: 108.5080795288086\n",
      "Epoch: 9700 | loss train: 103.9651870727539\n",
      "Epoch: 9800 | loss train: 111.55359649658203\n",
      "Epoch: 9900 | loss train: 122.4545669555664\n",
      "Epoch: 10000 | loss train: 100.5232925415039\n",
      "Epoch: 10100 | loss train: 101.74212646484375\n",
      "Epoch: 10200 | loss train: 106.52891540527344\n",
      "Epoch: 10300 | loss train: 94.01171875\n",
      "Epoch: 10400 | loss train: 98.63937377929688\n",
      "Epoch: 10500 | loss train: 103.107177734375\n",
      "Epoch: 10600 | loss train: 105.6917724609375\n",
      "Epoch: 10700 | loss train: 97.1581802368164\n",
      "Epoch: 10800 | loss train: 91.60309600830078\n",
      "Epoch: 10900 | loss train: 88.66478729248047\n",
      "Epoch: 11000 | loss train: 95.22537231445312\n",
      "Epoch: 11100 | loss train: 86.86612701416016\n",
      "Epoch: 11200 | loss train: 104.99964904785156\n",
      "Epoch: 11300 | loss train: 98.16946411132812\n",
      "Epoch: 11400 | loss train: 94.18095397949219\n",
      "Epoch: 11500 | loss train: 92.2145767211914\n",
      "Epoch: 11600 | loss train: 80.38958740234375\n",
      "Epoch: 11700 | loss train: 101.49020385742188\n",
      "Epoch: 11800 | loss train: 82.23155975341797\n",
      "Epoch: 11900 | loss train: 95.17045593261719\n",
      "Epoch: 12000 | loss train: 91.44284057617188\n",
      "Epoch: 12100 | loss train: 92.05980682373047\n",
      "Epoch: 12200 | loss train: 75.38330078125\n",
      "Epoch: 12300 | loss train: 88.43988037109375\n",
      "Epoch: 12400 | loss train: 77.95030212402344\n",
      "Epoch: 12500 | loss train: 103.4140853881836\n",
      "Epoch: 12600 | loss train: 87.48375701904297\n",
      "Epoch: 12700 | loss train: 82.19029235839844\n",
      "Epoch: 12800 | loss train: 92.82210540771484\n",
      "Epoch: 12900 | loss train: 79.69452667236328\n",
      "Epoch: 13000 | loss train: 84.7295913696289\n",
      "Epoch: 13100 | loss train: 76.54530334472656\n",
      "Epoch: 13200 | loss train: 72.26670837402344\n",
      "Epoch: 13300 | loss train: 109.00770568847656\n",
      "Epoch: 13400 | loss train: 66.39534759521484\n",
      "Epoch: 13500 | loss train: 70.7136459350586\n",
      "Epoch: 13600 | loss train: 90.76323699951172\n",
      "Epoch: 13700 | loss train: 73.3359603881836\n",
      "Epoch: 13800 | loss train: 70.41388702392578\n",
      "Epoch: 13900 | loss train: 89.71552276611328\n",
      "Epoch: 14000 | loss train: 63.28257751464844\n",
      "Epoch: 14100 | loss train: 63.041072845458984\n",
      "Epoch: 14200 | loss train: 61.97242736816406\n",
      "Epoch: 14300 | loss train: 63.449764251708984\n",
      "Epoch: 14400 | loss train: 79.20980834960938\n",
      "Epoch: 14500 | loss train: 62.18404006958008\n",
      "Epoch: 14600 | loss train: 59.007713317871094\n",
      "Epoch: 14700 | loss train: 61.867950439453125\n",
      "Epoch: 14800 | loss train: 61.40806579589844\n",
      "Epoch: 14900 | loss train: 60.79294204711914\n",
      "Epoch: 15000 | loss train: 58.25519561767578\n",
      "Epoch: 15100 | loss train: 58.20243453979492\n",
      "Epoch: 15200 | loss train: 58.61781692504883\n",
      "Epoch: 15300 | loss train: 68.82843780517578\n",
      "Epoch: 15400 | loss train: 82.7006607055664\n",
      "Epoch: 15500 | loss train: 54.461612701416016\n",
      "Epoch: 15600 | loss train: 61.467994689941406\n",
      "Epoch: 15700 | loss train: 54.131103515625\n",
      "Epoch: 15800 | loss train: 65.5946044921875\n",
      "Epoch: 15900 | loss train: 73.87615203857422\n",
      "Epoch: 16000 | loss train: 52.97514724731445\n",
      "Epoch: 16100 | loss train: 83.78145599365234\n",
      "Epoch: 16200 | loss train: 77.09589385986328\n",
      "Epoch: 16300 | loss train: 82.77587890625\n",
      "Epoch: 16400 | loss train: 119.5475082397461\n",
      "Epoch: 16500 | loss train: 89.99754333496094\n",
      "Epoch: 16600 | loss train: 49.867584228515625\n",
      "Epoch: 16700 | loss train: 58.30208206176758\n",
      "Epoch: 16800 | loss train: 48.56133270263672\n",
      "Epoch: 16900 | loss train: 48.36663055419922\n",
      "Epoch: 17000 | loss train: 49.35408020019531\n",
      "Epoch: 17100 | loss train: 48.42131805419922\n",
      "Epoch: 17200 | loss train: 97.9706802368164\n",
      "Epoch: 17300 | loss train: 68.75373840332031\n",
      "Epoch: 17400 | loss train: 61.33755111694336\n",
      "Epoch: 17500 | loss train: 47.64335632324219\n",
      "Epoch: 17600 | loss train: 45.5278434753418\n",
      "Epoch: 17700 | loss train: 46.97184371948242\n",
      "Epoch: 17800 | loss train: 53.92451095581055\n",
      "Epoch: 17900 | loss train: 45.41440200805664\n",
      "Epoch: 18000 | loss train: 106.33370971679688\n",
      "Epoch: 18100 | loss train: 99.3706283569336\n",
      "Epoch: 18200 | loss train: 65.33010864257812\n",
      "Epoch: 18300 | loss train: 45.993045806884766\n",
      "Epoch: 18400 | loss train: 43.67355728149414\n",
      "Epoch: 18500 | loss train: 46.712989807128906\n",
      "Epoch: 18600 | loss train: 46.6768913269043\n",
      "Epoch: 18700 | loss train: 44.59370422363281\n",
      "Epoch: 18800 | loss train: 41.58645248413086\n",
      "Epoch: 18900 | loss train: 120.7754135131836\n",
      "Epoch: 19000 | loss train: 84.2527847290039\n",
      "Epoch: 19100 | loss train: 40.97968673706055\n",
      "Epoch: 19200 | loss train: 40.947608947753906\n",
      "Epoch: 19300 | loss train: 44.48891067504883\n",
      "Epoch: 19400 | loss train: 41.601593017578125\n",
      "Epoch: 19500 | loss train: 44.94062423706055\n",
      "Epoch: 19600 | loss train: 43.648921966552734\n",
      "Epoch: 19700 | loss train: 40.377960205078125\n",
      "Epoch: 19800 | loss train: 41.96382141113281\n",
      "Epoch: 19900 | loss train: 49.70810317993164\n",
      "Epoch: 20000 | loss train: 37.90428924560547\n",
      "Epoch: 20100 | loss train: 40.359901428222656\n",
      "Epoch: 20200 | loss train: 37.44701385498047\n",
      "Epoch: 20300 | loss train: 40.12091827392578\n",
      "Epoch: 20400 | loss train: 36.946563720703125\n",
      "Epoch: 20500 | loss train: 44.48655319213867\n",
      "Epoch: 20600 | loss train: 49.675594329833984\n",
      "Epoch: 20700 | loss train: 36.33203887939453\n",
      "Epoch: 20800 | loss train: 48.33027648925781\n",
      "Epoch: 20900 | loss train: 36.92510986328125\n",
      "Epoch: 21000 | loss train: 37.1983642578125\n",
      "Epoch: 21100 | loss train: 40.26171875\n",
      "Epoch: 21200 | loss train: 35.56821060180664\n",
      "Epoch: 21300 | loss train: 36.80803298950195\n",
      "Epoch: 21400 | loss train: 37.45758819580078\n",
      "Epoch: 21500 | loss train: 35.40988540649414\n",
      "Epoch: 21600 | loss train: 61.499752044677734\n",
      "Epoch: 21700 | loss train: 53.87470245361328\n",
      "Epoch: 21800 | loss train: 62.34855270385742\n",
      "Epoch: 21900 | loss train: 48.00749588012695\n",
      "Epoch: 22000 | loss train: 37.02669906616211\n",
      "Epoch: 22100 | loss train: 32.88108444213867\n",
      "Epoch: 22200 | loss train: 53.2181282043457\n",
      "Epoch: 22300 | loss train: 39.430667877197266\n",
      "Epoch: 22400 | loss train: 32.673072814941406\n",
      "Epoch: 22500 | loss train: 32.25071716308594\n",
      "Epoch: 22600 | loss train: 32.80290985107422\n",
      "Epoch: 22700 | loss train: 77.92427062988281\n",
      "Epoch: 22800 | loss train: 91.18154907226562\n",
      "Epoch: 22900 | loss train: 105.15260314941406\n",
      "Epoch: 23000 | loss train: 36.14067459106445\n",
      "Epoch: 23100 | loss train: 30.578899383544922\n",
      "Epoch: 23200 | loss train: 35.70100402832031\n",
      "Epoch: 23300 | loss train: 30.779573440551758\n",
      "Epoch: 23400 | loss train: 30.876720428466797\n",
      "Epoch: 23500 | loss train: 29.622163772583008\n",
      "Epoch: 23600 | loss train: 90.84052276611328\n",
      "Epoch: 23700 | loss train: 34.66255569458008\n",
      "Epoch: 23800 | loss train: 29.149206161499023\n",
      "Epoch: 23900 | loss train: 36.8094367980957\n",
      "Epoch: 24000 | loss train: 28.550657272338867\n",
      "Epoch: 24100 | loss train: 48.7421875\n",
      "Epoch: 24200 | loss train: 60.34235382080078\n",
      "Epoch: 24300 | loss train: 38.575531005859375\n",
      "Epoch: 24400 | loss train: 120.33355712890625\n",
      "Epoch: 24500 | loss train: 67.2851333618164\n",
      "Epoch: 24600 | loss train: 27.9216365814209\n",
      "Epoch: 24700 | loss train: 69.81925201416016\n",
      "Epoch: 24800 | loss train: 27.273942947387695\n",
      "Epoch: 24900 | loss train: 93.78948974609375\n",
      "Epoch: 25000 | loss train: 48.69246292114258\n",
      "Epoch: 25100 | loss train: 33.46712875366211\n",
      "Epoch: 25200 | loss train: 26.38636016845703\n",
      "Epoch: 25300 | loss train: 65.5301742553711\n",
      "Epoch: 25400 | loss train: 26.18352699279785\n",
      "Epoch: 25500 | loss train: 25.980195999145508\n",
      "Epoch: 25600 | loss train: 75.47835540771484\n",
      "Epoch: 25700 | loss train: 27.812467575073242\n",
      "Epoch: 25800 | loss train: 55.25387954711914\n",
      "Epoch: 25900 | loss train: 73.55966186523438\n",
      "Epoch: 26000 | loss train: 101.21170043945312\n",
      "Epoch: 26100 | loss train: 74.05082702636719\n",
      "Epoch: 26200 | loss train: 24.03487777709961\n",
      "Epoch: 26300 | loss train: 47.16974639892578\n",
      "Epoch: 26400 | loss train: 24.58078384399414\n",
      "Epoch: 26500 | loss train: 23.5865535736084\n",
      "Epoch: 26600 | loss train: 23.724313735961914\n",
      "Epoch: 26700 | loss train: 94.64469146728516\n",
      "Epoch: 26800 | loss train: 22.936006546020508\n",
      "Epoch: 26900 | loss train: 75.31301879882812\n",
      "Epoch: 27000 | loss train: 24.34577751159668\n",
      "Epoch: 27100 | loss train: 22.6357421875\n",
      "Epoch: 27200 | loss train: 22.734342575073242\n",
      "Epoch: 27300 | loss train: 24.850669860839844\n",
      "Epoch: 27400 | loss train: 126.34441375732422\n",
      "Epoch: 27500 | loss train: 21.857629776000977\n",
      "Epoch: 27600 | loss train: 21.632312774658203\n",
      "Epoch: 27700 | loss train: 23.139005661010742\n",
      "Epoch: 27800 | loss train: 22.479122161865234\n",
      "Epoch: 27900 | loss train: 60.8327522277832\n",
      "Epoch: 28000 | loss train: 24.642498016357422\n",
      "Epoch: 28100 | loss train: 77.5761947631836\n",
      "Epoch: 28200 | loss train: 23.612043380737305\n",
      "Epoch: 28300 | loss train: 25.98337745666504\n",
      "Epoch: 28400 | loss train: 25.699228286743164\n",
      "Epoch: 28500 | loss train: 25.505990982055664\n",
      "Epoch: 28600 | loss train: 21.272884368896484\n",
      "Epoch: 28700 | loss train: 21.239269256591797\n",
      "Epoch: 28800 | loss train: 21.123788833618164\n",
      "Epoch: 28900 | loss train: 19.982826232910156\n",
      "Epoch: 29000 | loss train: 147.43226623535156\n",
      "Epoch: 29100 | loss train: 20.58342933654785\n",
      "Epoch: 29200 | loss train: 23.606794357299805\n",
      "Epoch: 29300 | loss train: 32.881649017333984\n",
      "Epoch: 29400 | loss train: 19.75861930847168\n",
      "Epoch: 29500 | loss train: 20.307443618774414\n",
      "Epoch: 29600 | loss train: 54.065895080566406\n",
      "Epoch: 29700 | loss train: 18.798635482788086\n",
      "Epoch: 29800 | loss train: 20.524784088134766\n",
      "Epoch: 29900 | loss train: 18.920116424560547\n",
      "Epoch: 30000 | loss train: 19.335500717163086\n",
      "Epoch: 30100 | loss train: 53.36442947387695\n",
      "Epoch: 30200 | loss train: 18.236083984375\n",
      "Epoch: 30300 | loss train: 26.326730728149414\n",
      "Epoch: 30400 | loss train: 34.1866455078125\n",
      "Epoch: 30500 | loss train: 17.86098289489746\n",
      "Epoch: 30600 | loss train: 33.79576110839844\n",
      "Epoch: 30700 | loss train: 30.715343475341797\n",
      "Epoch: 30800 | loss train: 21.991300582885742\n",
      "Epoch: 30900 | loss train: 17.995141983032227\n",
      "Epoch: 31000 | loss train: 17.76752471923828\n",
      "Epoch: 31100 | loss train: 42.55424499511719\n",
      "Epoch: 31200 | loss train: 18.19232177734375\n",
      "Epoch: 31300 | loss train: 19.847097396850586\n",
      "Epoch: 31400 | loss train: 31.685361862182617\n",
      "Epoch: 31500 | loss train: 27.484373092651367\n",
      "Epoch: 31600 | loss train: 17.294570922851562\n",
      "Epoch: 31700 | loss train: 17.339452743530273\n",
      "Epoch: 31800 | loss train: 19.258445739746094\n",
      "Epoch: 31900 | loss train: 16.75935173034668\n",
      "Epoch: 32000 | loss train: 17.2282772064209\n",
      "Epoch: 32100 | loss train: 92.65794372558594\n",
      "Epoch: 32200 | loss train: 17.950727462768555\n",
      "Epoch: 32300 | loss train: 18.212453842163086\n",
      "Epoch: 32400 | loss train: 18.028039932250977\n",
      "Epoch: 32500 | loss train: 18.193605422973633\n",
      "Epoch: 32600 | loss train: 61.65623092651367\n",
      "Epoch: 32700 | loss train: 15.734696388244629\n",
      "Epoch: 32800 | loss train: 15.684916496276855\n",
      "Epoch: 32900 | loss train: 49.49596405029297\n",
      "Epoch: 33000 | loss train: 15.234910011291504\n",
      "Epoch: 33100 | loss train: 15.683777809143066\n",
      "Epoch: 33200 | loss train: 41.2635383605957\n",
      "Epoch: 33300 | loss train: 14.844701766967773\n",
      "Epoch: 33400 | loss train: 15.981019020080566\n",
      "Epoch: 33500 | loss train: 15.2471342086792\n",
      "Epoch: 33600 | loss train: 14.448173522949219\n",
      "Epoch: 33700 | loss train: 258.38421630859375\n",
      "Epoch: 33800 | loss train: 14.399941444396973\n",
      "Epoch: 33900 | loss train: 14.173685073852539\n",
      "Epoch: 34000 | loss train: 18.130014419555664\n",
      "Epoch: 34100 | loss train: 26.633859634399414\n",
      "Epoch: 34200 | loss train: 13.921544075012207\n",
      "Epoch: 34300 | loss train: 28.311965942382812\n",
      "Epoch: 34400 | loss train: 14.62770938873291\n",
      "Epoch: 34500 | loss train: 14.606668472290039\n",
      "Epoch: 34600 | loss train: 121.78591918945312\n",
      "Epoch: 34700 | loss train: 13.728802680969238\n",
      "Epoch: 34800 | loss train: 29.400333404541016\n",
      "Epoch: 34900 | loss train: 14.184476852416992\n",
      "Epoch: 35000 | loss train: 46.08216094970703\n",
      "Epoch: 35100 | loss train: 13.074033737182617\n",
      "Epoch: 35200 | loss train: 12.870180130004883\n",
      "Epoch: 35300 | loss train: 17.26753044128418\n",
      "Epoch: 35400 | loss train: 13.92078685760498\n",
      "Epoch: 35500 | loss train: 12.921187400817871\n",
      "Epoch: 35600 | loss train: 12.623263359069824\n",
      "Epoch: 35700 | loss train: 35.56585693359375\n",
      "Epoch: 35800 | loss train: 12.67684268951416\n",
      "Epoch: 35900 | loss train: 12.42399787902832\n",
      "Epoch: 36000 | loss train: 14.704237937927246\n",
      "Epoch: 36100 | loss train: 15.399904251098633\n",
      "Epoch: 36200 | loss train: 11.864142417907715\n",
      "Epoch: 36300 | loss train: 35.36304473876953\n",
      "Epoch: 36400 | loss train: 12.244209289550781\n",
      "Epoch: 36500 | loss train: 12.563569068908691\n",
      "Epoch: 36600 | loss train: 98.80886840820312\n",
      "Epoch: 36700 | loss train: 11.837191581726074\n",
      "Epoch: 36800 | loss train: 13.825911521911621\n",
      "Epoch: 36900 | loss train: 11.346214294433594\n",
      "Epoch: 37000 | loss train: 14.407330513000488\n",
      "Epoch: 37100 | loss train: 121.58878326416016\n",
      "Epoch: 37200 | loss train: 11.444686889648438\n",
      "Epoch: 37300 | loss train: 10.97406005859375\n",
      "Epoch: 37400 | loss train: 14.679455757141113\n",
      "Epoch: 37500 | loss train: 54.858760833740234\n",
      "Epoch: 37600 | loss train: 19.567537307739258\n",
      "Epoch: 37700 | loss train: 11.059004783630371\n",
      "Epoch: 37800 | loss train: 11.001468658447266\n",
      "Epoch: 37900 | loss train: 11.124558448791504\n",
      "Epoch: 38000 | loss train: 45.45418930053711\n",
      "Epoch: 38100 | loss train: 10.612603187561035\n",
      "Epoch: 38200 | loss train: 14.782150268554688\n",
      "Epoch: 38300 | loss train: 90.21011352539062\n",
      "Epoch: 38400 | loss train: 10.135217666625977\n",
      "Epoch: 38500 | loss train: 17.91654396057129\n",
      "Epoch: 38600 | loss train: 10.104191780090332\n",
      "Epoch: 38700 | loss train: 10.92186164855957\n",
      "Epoch: 38800 | loss train: 9.879467010498047\n",
      "Epoch: 38900 | loss train: 10.097373962402344\n",
      "Epoch: 39000 | loss train: 9.688200950622559\n",
      "Epoch: 39100 | loss train: 28.442676544189453\n",
      "Epoch: 39200 | loss train: 10.467742919921875\n",
      "Epoch: 39300 | loss train: 9.762697219848633\n",
      "Epoch: 39400 | loss train: 70.80719757080078\n",
      "Epoch: 39500 | loss train: 9.518609046936035\n",
      "Epoch: 39600 | loss train: 106.74988555908203\n",
      "Epoch: 39700 | loss train: 9.385686874389648\n",
      "Epoch: 39800 | loss train: 17.152542114257812\n",
      "Epoch: 39900 | loss train: 9.270892143249512\n",
      "loss_NN=tensor(1037.2438)\n",
      "pred_NN=tensor([[5342.4971],\n",
      "        [5613.6509],\n",
      "        [6817.6348],\n",
      "        [6248.8032]])\n",
      "loss_RF=936.7041095482797\n",
      "pred_RF=array([5934.73453125, 5832.21734863, 6421.3114209 , 5794.84550781])\n",
      "loss_DT=1603.5282813868748\n",
      "pred_DT=array([6336.36523438, 5037.17431641, 5546.46972656, 4766.24169922])\n"
     ]
    }
   ],
   "source": [
    "sample = generate_sample(data_df_13_09_2022, bands_ix, target_value, train_field)\n",
    "X_train, Y_train = create_X_train_Y_train(sample, bands_ix)\n",
    "sample_target = generate_sample(data_df_13_09_2022, bands_ix, target_value, \"BC\")\n",
    "X_target, Y_target = create_X_train_Y_train(sample_target, bands_ix)\n",
    "super_param={\"lr\": 0.0001, \"weight_decay\": 1e-5, \"n_epochs\": 40000, \"stop_value\": 130}\n",
    "re_run = \"Y\"\n",
    "loss_NN, pred_NN, NN_model = predict_using_neutral_network(\n",
    "    X_train, \n",
    "    Y_train, \n",
    "    X_target, \n",
    "    Y_target, \n",
    "    bands_ix,\n",
    "    NN_file_path, \n",
    "    super_param, \n",
    "    re_run)\n",
    "print(f\"{loss_NN=}\")\n",
    "print(f\"{pred_NN=}\")\n",
    "loss_RF, pred_RF, RF_model = predict_using_random_forest(X_train, Y_train, X_target, Y_target, bands_ix, super_param)\n",
    "print(f\"{loss_RF=}\")\n",
    "print(f\"{pred_RF=}\")\n",
    "loss_DT, pred_DT, DT_model = predict_using_decision_tree(X_train, Y_train, X_target, Y_target, bands_ix, super_param)\n",
    "print(f\"{loss_DT=}\")\n",
    "print(f\"{pred_DT=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save NN object to: D:\\code\\model_saved\\NN_save\\NN_object\\T_predict_K_2024-03-07-22-56-00_using_get_max_bands.pkl\n",
      "Save RF object to: D:\\code\\model_saved\\RF_save\\T_predict_K_2024-03-07-22-56-00_using_get_max_bands.pkl\n",
      "Save DT object to: D:\\code\\model_saved\\DT_save\\T_predict_K_2024-03-07-22-56-00_using_get_max_bands.pkl\n"
     ]
    }
   ],
   "source": [
    "cloudpickle.register_pickle_by_value(common)\n",
    "\n",
    "with open(NN_file_path, \"wb\") as file:\n",
    "    cloudpickle.dump(NN_model, file)\n",
    "    print(f\"Save NN object to: {NN_file_path}\")\n",
    "with open(RF_file_path, \"wb\") as file:\n",
    "    cloudpickle.dump(RF_model, file)\n",
    "    print(f\"Save RF object to: {RF_file_path}\")\n",
    "with open(DT_file_path, \"wb\") as file:\n",
    "    cloudpickle.dump(DT_model, file)\n",
    "    print(f\"Save DT object to: {DT_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "avarage_target_value = np.average(Y_target)\n",
    "loss_NN = float(loss_NN)\n",
    "df_report = pd.DataFrame(\n",
    "    {\n",
    "        \"date_time\": [datetime.now()],\n",
    "        \"train_field\": [str(train_field)],\n",
    "        \"target_value\": [str(target_value)],\n",
    "        \"bands_ix\": [str(bands_ix)],\n",
    "        \"total_band\": [len(bands_ix)],\n",
    "        \"function_get\": [function_get.__name__],\n",
    "        \"loss_NN\": [loss_NN],\n",
    "        \"% loss_NN\": [loss_NN / avarage_target_value * 100],\n",
    "        \"loss_RF\": [loss_RF],\n",
    "        \"% loss_RF\": [loss_RF / avarage_target_value * 100],\n",
    "        \"loss_DT\": [loss_DT],\n",
    "        \"% loss_DT\": [loss_DT / avarage_target_value * 100],\n",
    "        \"average_target_value\": [avarage_target_value],\n",
    "        \"File save NN\": [NN_file_path],\n",
    "        \"File save RF\": [RF_file_path],\n",
    "        \"File save DT\": [DT_file_path],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_time</th>\n",
       "      <th>train_field</th>\n",
       "      <th>target_value</th>\n",
       "      <th>bands_ix</th>\n",
       "      <th>total_band</th>\n",
       "      <th>function_get</th>\n",
       "      <th>loss_NN</th>\n",
       "      <th>% loss_NN</th>\n",
       "      <th>loss_RF</th>\n",
       "      <th>% loss_RF</th>\n",
       "      <th>loss_DT</th>\n",
       "      <th>% loss_DT</th>\n",
       "      <th>average_target_value</th>\n",
       "      <th>File save NN</th>\n",
       "      <th>File save RF</th>\n",
       "      <th>File save DT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-03-07 23:02:15.481179</td>\n",
       "      <td>T</td>\n",
       "      <td>K</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>122</td>\n",
       "      <td>get_max_bands</td>\n",
       "      <td>4755.72998</td>\n",
       "      <td>37.707614</td>\n",
       "      <td>3506.90181</td>\n",
       "      <td>27.805805</td>\n",
       "      <td>3099.819108</td>\n",
       "      <td>24.578095</td>\n",
       "      <td>12612.121094</td>\n",
       "      <td>D:\\code\\model_saved\\NN_save\\NN_object\\T_predic...</td>\n",
       "      <td>D:\\code\\model_saved\\RF_save\\T_predict_K_2024-0...</td>\n",
       "      <td>D:\\code\\model_saved\\DT_save\\T_predict_K_2024-0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   date_time train_field target_value  \\\n",
       "0 2024-03-07 23:02:15.481179           T            K   \n",
       "\n",
       "                                            bands_ix  total_band  \\\n",
       "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...         122   \n",
       "\n",
       "    function_get     loss_NN  % loss_NN     loss_RF  % loss_RF      loss_DT  \\\n",
       "0  get_max_bands  4755.72998  37.707614  3506.90181  27.805805  3099.819108   \n",
       "\n",
       "   % loss_DT  average_target_value  \\\n",
       "0  24.578095          12612.121094   \n",
       "\n",
       "                                        File save NN  \\\n",
       "0  D:\\code\\model_saved\\NN_save\\NN_object\\T_predic...   \n",
       "\n",
       "                                        File save RF  \\\n",
       "0  D:\\code\\model_saved\\RF_save\\T_predict_K_2024-0...   \n",
       "\n",
       "                                        File save DT  \n",
       "0  D:\\code\\model_saved\\DT_save\\T_predict_K_2024-0...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_df_to_excel(df: pd.DataFrame, file_path, sheet_name=\"Sheet1\"):\n",
    "    if not os.path.exists(file_path):\n",
    "        df.to_excel(\n",
    "            file_path,\n",
    "            sheet_name=sheet_name,\n",
    "            index=False,\n",
    "            header=True\n",
    "        )\n",
    "        return\n",
    "    \n",
    "    wb = load_workbook(file_path)\n",
    "    ws = wb[sheet_name]\n",
    "    for r in dataframe_to_rows(df, index=False, header=False):\n",
    "        ws.append(r)\n",
    "    wb.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: DATA IS WRITED TO FILE D:\\code\\report\\agriculture_report.xlsx\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    report_file_path = get_full_path(\"../../report/agriculture_report.xlsx\")\n",
    "    sheet_name = \"Sheet1\"\n",
    "\n",
    "    append_df_to_excel(df_report, report_file_path, sheet_name)\n",
    "    print(f\"SUCCESS: DATA IS WRITED TO FILE {report_file_path}\")\n",
    "except PermissionError as err:\n",
    "    print(f\"ERROR: YOU ARE OPENNING FILE {report_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
